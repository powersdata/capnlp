---
title: "Natual Language - Predictive Modeling"
date: "`r format(Sys.time(), ' %B %d, %Y')`"
output: html_document
---
Explore new models and data to improve your predictive model.
Evaluate your new predictions on both accuracy and efficiency.
Questions to consider

What are some alternative data sets you could consider using?
What are ways in which the n-gram model may be inefficient?
What are the most commonly missed n-grams? Can you think of a reason why they would be missed and fix that?
What are some other things that other people have tried to improve their model?
Can you estimate how uncertain you are about the words you are predicting?

###Overview
Typing is a common practice for inputing information into an electronic gadget.  Companies like SwiftKey are enhancing the interaction between people and technology by creating intuitive and personalized keyboard software to predict the next word or phrase.  [Data](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) from twitter, news and blogs was accessed and explored during this processing of creating a predictive model that reduces the keystrokes required for the entry of common words and phrases into an electronic gadget.

```{r, echo=F, message=FALSE, warning=FALSE, cache = FALSE, results="hide"}
which_function <- "main"
source("scripts/005functions.R")
#library(wordcloud)
#library(RColorBrewer)
#library(ggplot2)
```

###Exploring the Data

The data that was used to create the model can be found on the web at https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

```{r, echo=F, message=FALSE, warning=FALSE, cache = FALSE, results="hide"}
which_import <- "main"
source("scripts/010imports.R")

# if (!dir.exists("data")) {
#         dir.create("data")
#         }
#         if (!dir.exists("data/sample")) {
#         dir.create("data/sample")
#         }
#         
#         fileUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
#         filename <- basename(fileUrl)
#         
#         if (!file.exists(paste0("data/", filename))) {
#         setInternet2(use = TRUE)
#         download.file(fileUrl, destfile = paste0("data/", filename))
#         dateDownloaded <- date()
#         write(dateDownloaded, file = "data/dateDownloaded.txt")
#         zip_ls = unzip(paste0("data/", filename), list = TRUE)
#         unzip (
#         paste0("data/", filename), files = zip_ls[11:13,1], junkpaths = FALSE, exdir = "./data"
#         )
#         }
#         
#         twitter <- "en_US.twitter.txt"
#         news <- "en_US.news.txt"
#         blog <- "en_US.blogs.txt"
#         rm(fileUrl
#         )
```



```{r, echo=FALSE, eval=FALSE, comment=NA, results="hide"}
###functions used to create sample data that would not Knit
#counts the number of lines in a text file
# no_of_lines <- function(con_text){
#         readsizeof <- 12000
#         no_lines <- 0
#         con <- file(paste0("data/final/en_US/", con_text), "r")
#         ( while((linesread <- length(readLines(con ,readsizeof))) > 0 )
#                 no_lines <- no_lines+linesread )
#         closeAllConnections() 
#         no_lines
# }

#writes sample of text file lines to data for training
# write_training_text <- function(writefile){
#         starttime <- Sys.time()
#         
#         if (writefile == "en_US.twitter.txt") {samplelines <- 100; sourceletter <- "t"}    
#         if (writefile == "en_US.news.txt")    {samplelines <- 100; sourceletter <- "n"} 
#         if (writefile == "en_US.blogs.txt")   {samplelines <- 100; sourceletter <- "b"} 
#         
#         size <- no_of_lines(writefile)
#         #samplelines <- ifelse (size < 30000, size * 0.6, 29999)
#         rlines <- rbinom(samplelines, size, .5)
#         con <- file(paste0("data/final/en_US/", writefile), "r")
#         wlines <- readLines(con, encoding = "UTF-8")[rlines]
#         closeAllConnections()
#         save_source_RDS(filename = wlines, sourceletter, samplelines)
#         write.csv(wlines, file = paste0("data/sample/", basename(writefile)), row.names = FALSE)
#         closeAllConnections()
#         print(paste("TIME for", writefile, "training is", Sys.time() - starttime))
# }

set.seed(42)  #for reproducibility

#write sample data to disk
write_training_text(twitter)
file.copy(from = paste0("data/sample/", basename(twitter)), to = "data/sample/twitter", recursive = TRUE)
write_training_text(news)
file.copy(from = paste0("data/sample/", basename(news)), to = "data/sample/news", recursive = TRUE)
write_training_text(blog)
file.copy(from = paste0("data/sample/", basename(blog)), to = "data/sample/blogs", recursive = TRUE)

rm(filename, no_of_lines, write_training_text, .Random.seed)
```

- **Cleanse Data**

Before finding the most common words, the data is cleansed of undesired words as well as non-words like numbers, websites and emails.  All extra white space and puncuation is also removed but not before converting contractions to their respective words.  Frontmedia provided a list of undesired words from their site called  [Terms-to-Block.csv](http://www.frontgatemedia.com/a-list-of-723-bad-words-to-blacklist-and-how-to-use-facebooks-moderation-tool/).  You will see a table for the sample data below.  


Here is the detail code for cleansing the data:

```{r}
#clean corpus
clean_corpus <- function(type) {
 #Removed extra whitespace
type <- tm_map(type, stripWhitespace)
type <- tm_map(type, stripWhitespace)
#Transformed all characters to lowercase
type <- tm_map(type, content_transformer(tolower))

#Remove undesired words
profanity <- read.delim("data/Terms-to-Block.csv", skip = 4, header=FALSE, stringsAsFactors=FALSE, sep = ',', fill = TRUE)
profanity <- profanity[,2]
profanity <- gsub(",$","",profanity)
type <- tm_map(type, removeWords, profanity)

#Remove select non words like websites & emails
nonwords <- c("\\bhttp(\\S*)\\b" ,"\\b(\\S*)?\\.com\\b", "\\b(\\S*)?\\.gov\\b", "\\b(\\S*)?\\.org\\b", "\\b(\\S*)?\\.net\\b", "\\b(\\S*)?\\.tv\\b") 
type <- tm_map(type, removeWords, nonwords)

#Update word contractions and anything that is not a letter, number or space
contractions <-
        data.frame(
        rplmnt = c("s", " is", " am", "an not", " not", " will", " are", " would", " have",""),
        ptrn = c("s[^a-z0-9\\s]s", "\\b[^a-z0-9\\s]s\\b", "\\b[^a-z0-9\\s]m\\b", "an[^a-z0-9\\s]t", 
                 "n[^a-z0-9\\s]t", "\\b[^a-z0-9\\s]ll\\b", "\\b[^a-z0-9\\s]re\\b", "\\b[^a-z0-9\\s]d\\b",
                 "\\b[^a-z0-9\\s]ve\\b", "[^a-z\\s]"), stringsAsFactors = FALSE
                    ) 

for (j in 1:length(type)) {
        for (i in 1:nrow(contractions)) {
        type[[j]]$content <-
        gsub(contractions[i,2], contractions[i,1], type[[j]]$content, perl = TRUE)
        }
        }
        
#Remove Numbers
type <- tm_map(type, removeNumbers)

#Remove Punctuation
type<- tm_map(type, removePunctuation)

#Remove whitespace and profanity again
type <- tm_map(type, removeWords, profanity)
type <- tm_map(type, stripWhitespace)
type
}
```
 
- **Bigrams and Trigrams**

```{r, echo=F, message=FALSE, warning=FALSE, cache = FALSE, results="markup", fig.width=4.2, fig.height=4}
corpus_to_ngram <- function(corpus) {
        starttime <- Sys.time()
        corpus_ngram <- NULL
        for (i in 2:6) {
                        tmp_ngram <- NGramTokenizer(corpus, Weka_control(min = i, max = i))
                        xgram <- data.frame(table(tmp_ngram), stringsAsFactors = FALSE)
                        corpus_ngram <- rbind(corpus_ngram, xgram)
                        }
        #corpus_ngram <- NGramTokenizer(corpus, Weka_control(min = 2, max = ngram))
        
        colnames(corpus_ngram) <- c("nGram","Freq")
        corpus_ngram$nWords <- nchar(as.character(corpus_ngram$nGram)) + 1 - nchar(gsub(" ", "", as.character(corpus_ngram$nGram)))
        corpus_ngram <- corpus_ngram[order(corpus_ngram$Freq, decreasing = TRUE),]
        rownames(corpus_ngram) <- NULL
        print(paste("TIME for", corpus, "file is", Sys.time() - starttime))
        return(corpus_ngram)
}

#bigram <- corpus_to_ngram(sample_corpus, 2)
#trigram <- corpus_to_ngram(sample_corpus, 3)
#a <- clean_corpus(a)
```

```{r, echo=F, message=FALSE, warning=FALSE, cache = FALSE, results="markup"}
#a <- Corpus(DirSource("data/sample"), readerControl = list(language="UTF-8"))           #all
b <- read_source_RDS(sourceletter = "b", lines = "100")
n <- read_source_RDS(sourceletter = "n", lines = "100")
t <- read_source_RDS(sourceletter = "t", lines = "100")

# b <- Corpus(DirSource("data/sample/blogs"), readerControl = list(language="UTF-8"))     #blog
# n <- Corpus(DirSource("data/sample/news"), readerControl = list(language="UTF-8"))      #news
# t <- Corpus(DirSource("data/sample/twitter"), readerControl = list(language="UTF-8"))   #twitter

b <- Corpus(VectorSource(b), readerControl = list(language="UTF-8"))    #blog
n <- Corpus(VectorSource(n), readerControl = list(language="UTF-8"))    #news
t <- Corpus(VectorSource(t), readerControl = list(language="UTF-8"))    #twitter

b <- clean_corpus(b)
n <- clean_corpus(n)
t <- clean_corpus(t)

#a_quadgram <- corpus_to_ngram(a, 4)
b_quadgram <- corpus_to_ngram(b)
saveRDS(b_quadgram, file='b.rds')
rm(b_quadgram,b)

n_quadgram <- corpus_to_ngram(n)
saveRDS(n_quadgram, file='n.rds')
rm(n_quadgram,n)

t_quadgram <- corpus_to_ngram(t)
saveRDS(t_quadgram, file='t.rds')
rm(t_quadgram,t)
```

```{r}
b <- readRDS('b.rds')
n <- readRDS('n.rds')
t <- readRDS('t.rds')

a <- rbind(b,n,t)
con <- gzfile("women.rds")
readRDS(con)
close(con)
```

```{r, echo=FALSE}
# f <- function(queryHistoryTab, query, n = 2) {
#   require(tau)
#   trigrams <- sort(textcnt(rep(tolower(names(queryHistoryTab)), queryHistoryTab), method = "string", n = length(scan(text = query, what = "character", quiet = TRUE)) + 1))
#   query <- tolower(query)
#   idx <- which(substr(names(trigrams), 0, nchar(query)) == query)
#   res <- head(names(sort(trigrams[idx], decreasing = TRUE)), n)
#   res <- substr(res, nchar(query) + 2, nchar(res))
#   return(res)
# }
# f(c("Can of beer" = 3, "can of Soda" = 2, "A can of water" = 1, "Buy me a can of soda, please" = 2), "i do ")
# [1] "soda" "beer"
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
```{r, echo=F, message=FALSE, warning=FALSE, cache = FALSE, results="markup", fig.width=4.2, fig.height=4}
#find text

#word_seq <- query
#model_type <- "n"
#compare to quadrigram
qf <- function(model_type = "t", word_seq = "a"){
        word_count <- nchar(as.character(word_seq)) + 1 - nchar(gsub(" ", "", as.character(word_seq)))
        if (model_type == "t") gram <- t       
        if (model_type == "n") gram <- n
        if (model_type == "b") gram <- b
        if (model_type == "a") gram <- a
        gram <- gram[gram$nWords > word_count,]
        idx <- grepl(word_seq, gram$nGram, ignore.case = TRUE)
        #idx <- startsWith(gram$nGram, word_seq, ignore.case = TRUE)
        xgram <- gram[idx,]
        xgram <- xgram[order(xgram$Freq, decreasing = TRUE),]
        head(xgram, 10)
}
query <- "to the"
qf("a",query)
qf("b",query)
qf("n",query)
qf("t",query)


```

```{r, echo=F, message=FALSE, warning=FALSE, cache = FALSE, results="markup", fig.width=4.2, fig.height=4}
#find text
#query <- "of beer"
#compare to quadrigram
f<- function(query){

for (j in 1:length(type)) {
        for (i in 1:nrow(contractions)) {
        type[[j]]$content <-
        gsub(contractions[i,2], contractions[i,1], type[[j]]$content, perl = TRUE)
        }
        }
        
        idx <- grepl(query, quadgram$nGram, ignore.case = TRUE)

#sort quad based on text and frequency
xgram <- quadgram[idx,]
xgram <- xgram[order(xgram$Freq, decreasing = TRUE),]
head(xgram, 10)
}


be

insane

insensitive

asleep

callous

f(" insane")
f(" insensitive")
f(" asleep")
f(" callous")

```
For each of the sentence fragments below use your natural language processing algorithm to predict the next word in the sentence.

When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd

give

#die

eat

sleep
1
point
2. 
Guy at my table's wife got up to go to the bathroom and I asked about dessert and he started telling me about his

financial

spiritual

horticultural

#marital
1
point
3. 
I'd give anything to see arctic monkeys this

month

morning

#weekend

decade
1
point
4. 
Talking to your mom has the same effect as a hug and helps reduce your

hunger

sleepiness

happiness

#stress
1
point
5. 
When you were in Holland you were like 1 inch away from me but you hadn't time to take a

look

#picture

walk

minute
1
point
6. 
I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the

#matter

account

incident

case
1
point
7. 
I can't deal with unsymetrical things. I can't even hold an uneven number of bags of groceries in each

finger

#hand

toe

arm
1
point
8. 
Every inch of you is perfect from the bottom to the

#top

middle

side

center
1
point
9. 
I’m thankful my childhood was filled with imagination and bruises from playing

daily

weekly

inside

#outside
1
point
10. 
I like how the same people are in almost all of Adam Sandler's

stories

#movies

pictures

novels
